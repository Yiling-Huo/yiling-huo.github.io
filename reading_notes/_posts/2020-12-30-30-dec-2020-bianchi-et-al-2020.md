---
layout: post-narrow
title: 30 Dec 2020, Bianchi et al. (2020)
date: 2020-12-30 18:04
author: Yiling Huo
categories: Reading_notes
---
<!-- wp:paragraph -->
<p>This is a study comparing cloze-probability with several NLP techniques (a N-gram model, Latent Semantic Analysis LSA, Word2Vec, FastText) in predicting eye movement (gaze duration).</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>They used a N-gram + cache model (cache: train a additional N-gram model using the text read up to the present word to capture recent information). </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>For the word embedding models, they used cosine similarity as the computer-predictability (the semantic distance between the actual word and the predicted word I guess). </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>They analyzed how each of the models explained eye-tracking data of Spanish short stories, focusing on gaze duration. </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>They did analysis with predictability of the current word and the N+1 word. Besides cloze-probability, 4-gram + cache was the best performing model. The N-gram model also correlated with gaze duration in the same direction as cloze-predictability. The authors also argued that the word embedding models were likely to capture long-term effects (in long texts?).</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>I’m not sure that I fully understood what this paper is trying to say, but I’m glad to have some knowledge on NLP.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p><strong>References </strong></p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>Bianchi, B., Monzón, G. B., Ferrer, L., Slezak, D. F., Shalom, D. E., &amp; Kamienkowski, J. E. (2020). Human and computer estimations of Predictability of words in written language. <em>Scientific reports</em>, <em>10</em>(1), 1-11.</p>
<!-- /wp:paragraph -->
